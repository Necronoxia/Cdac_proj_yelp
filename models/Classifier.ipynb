{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc78abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>7</td><td>application_1647103901388_0008</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-71-58.ec2.internal:20888/proxy/application_1647103901388_0008/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-74-218.ec2.internal:8042/node/containerlogs/container_1647103901388_0008_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '15g'), ('spark.driver.memory','15g'),(\"spark.driver.maxResultSize\",\"0\"),(\"spark.executor.cores\", '4'),(\"spark.dynamicAllocation.enabled\",\"False\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "978f9b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml import Transformer, classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57e159a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = sqlContext.read.parquet(\"s3://ash-proj/Parquet/yelp_academic_dataset_review_featuresNLabels/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "699c79fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.repartition(1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb255ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gbt = GBTClassifier(featuresCol='features',labelCol=\"label\")\n",
    "#model = DecisionTree.trainClassifier(sc.parallelize(df), 5, {})\n",
    "#NB = NaiveBayes(featuresCol='features', labelCol='label', smoothing=1.0, modelType='multinomial', thresholds=None, weightCol=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6df18521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o100.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 419, ip-172-31-68-64.ec2.internal, executor 2): java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n",
      "\tat scala.Predef$.require(Predef.scala:224)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1$$anonfun$1$$anonfun$apply$1.apply(GBTClassifier.scala:167)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1$$anonfun$1$$anonfun$apply$1.apply(GBTClassifier.scala:165)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:394)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2136)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2124)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2123)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2123)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:994)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:994)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2384)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2333)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2322)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:805)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2111)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2132)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1409)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.take(RDD.scala:1382)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1423)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.first(RDD.scala:1422)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1.apply(GBTClassifier.scala:183)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1.apply(GBTClassifier.scala:156)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:156)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n",
      "\tat scala.Predef$.require(Predef.scala:224)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1$$anonfun$1$$anonfun$apply$1.apply(GBTClassifier.scala:167)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1$$anonfun$1$$anonfun$apply$1.apply(GBTClassifier.scala:165)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:394)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 295, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 292, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o100.fit.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 419, ip-172-31-68-64.ec2.internal, executor 2): java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n",
      "\tat scala.Predef$.require(Predef.scala:224)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1$$anonfun$1$$anonfun$apply$1.apply(GBTClassifier.scala:167)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1$$anonfun$1$$anonfun$apply$1.apply(GBTClassifier.scala:165)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:394)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2136)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2124)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2123)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2123)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:994)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:994)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2384)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2333)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2322)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:805)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2111)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2132)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1409)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.take(RDD.scala:1382)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1423)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.first(RDD.scala:1422)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1.apply(GBTClassifier.scala:183)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1.apply(GBTClassifier.scala:156)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:156)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n",
      "\tat scala.Predef$.require(Predef.scala:224)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1$$anonfun$1$$anonfun$apply$1.apply(GBTClassifier.scala:167)\n",
      "\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$train$1$$anonfun$1$$anonfun$apply$1.apply(GBTClassifier.scala:165)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:394)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbt_model = gbt.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddce223",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rf_model.transform(df_train1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
