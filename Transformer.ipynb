{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbddfc74-3a89-4e30-8d3f-033db38caa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040d9e91-fd1d-4e82-a3ac-f9252037c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, ff_dim, n_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = layers.MultiHeadAtt(n_heads,key_dim=embed_dim)\n",
    "        self.ffnn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation = \"relu\"),\n",
    "             layers.Dense(embed_dim)]\n",
    "        )\n",
    "        \n",
    "        self.norm1 = layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.drop1 = layers.Dropout(0.1)\n",
    "        self.drop2 = layers.Dropout(0.1)\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        attention_out = self.attention(inputs, inputs)\n",
    "        attention_out = self.drop1(attention_out, training=training)\n",
    "        out1 = self.norm1(inputs + attention_out)\n",
    "        ffnn_out = self.ffnn(out1)\n",
    "        ffnn_out = self.dropout2(ffnn_out, training=training)\n",
    "        return self.norm2(out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f756940-25a4-4cca-8dd5-d137a1cf2327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPosEmbed(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPosEmbed, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocal_size, output_dim = embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        \n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9057c7b9-7ae2-465f-91b1-ac18a21bd64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "maxlen = 200\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d358204-7a2e-4cbf-bc5e-b5a64b485496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d2173e-025b-4c36-a5c1-e1a778d23025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddffb9a-c899-48ef-929b-c295ac13fe1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d662c834-18bc-4c79-ad80-b86e807b54e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfenv] *",
   "language": "python",
   "name": "conda-env-tfenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
