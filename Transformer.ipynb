{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbddfc74-3a89-4e30-8d3f-033db38caa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "040d9e91-fd1d-4e82-a3ac-f9252037c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, ffnn_dim, n_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = layers.MultiHeadAttention(n_heads,key_dim=embed_dim)\n",
    "        self.ffnn = keras.Sequential(\n",
    "            [layers.Dense(ffnn_dim, activation = \"relu\"),\n",
    "             layers.Dense(128, activation = \"relu\"),\n",
    "             layers.Dense(embed_dim)]\n",
    "        )\n",
    "        \n",
    "        self.norm1 = layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.drop1 = layers.Dropout(0.1)\n",
    "        self.drop2 = layers.Dropout(0.1)\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        attention_out = self.attention(inputs, inputs)\n",
    "        attention_out = self.drop1(attention_out, training=training)\n",
    "        out1 = self.norm1(inputs + attention_out)\n",
    "        ffnn_out = self.ffnn(out1)\n",
    "        ffnn_out = self.drop2(ffnn_out, training=training)\n",
    "        return self.norm2(out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f756940-25a4-4cca-8dd5-d137a1cf2327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPosEmbed(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPosEmbed, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim = embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        \n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9057c7b9-7ae2-465f-91b1-ac18a21bd64a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-77347d19e139>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# [0,1,2],[1,2,3]. Assuming array 3 is maxlen, i.e maxlen = 2.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "maxlen = 200\n",
    "\n",
    "# test train split\n",
    "\n",
    "# pad_sequence adds padding to each array suppose [1],[1,2],[1,2,3] are three\n",
    "# arrays then pad_sequence will make it such that the output is [0,0,1],\n",
    "# [0,1,2],[1,2,3]. Assuming array 3 is maxlen, i.e maxlen = 2.\n",
    "\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1bfcd0-0825-4331-8841-d5d0d3b6a108",
   "metadata": {},
   "source": [
    "## Classifier call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82d2173e-025b-4c36-a5c1-e1a778d23025",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 64 #embed size \n",
    "ffnn_dim = 64 #Neural network input dimension. Same as embed_dim\n",
    "num_heads = 4\n",
    "\n",
    "input_strm = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPosEmbed(maxlen, vocab_size, embed_dim)\n",
    "nn = embedding_layer(input_strm)\n",
    "transformer = TransformerBlock(embed_dim = embed_dim,ffnn_dim = ffnn_dim,n_heads = num_heads)(nn)\n",
    "nn = layers.GlobalAveragePooling1D()(nn)\n",
    "nn = layers.Dropout(0.1)(nn)\n",
    "nn = layers.Dense(128, activation='relu')(nn)\n",
    "nn = layers.Dropout(0.1)(nn)\n",
    "outputs = layers.Dense(2, activation = \"softmax\")(nn)\n",
    "\n",
    "cybertron = keras.Model(inputs = input_strm, outputs = outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5390e774-330b-4b45-9086-f7944082068f",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9d0e2e4-df48-4176-b59e-2262d856c3d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b07018e9c9f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m history = cybertron.fit(\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "cybertron.compile(optimizer = \"adam\", loss=\"categorical_crossentropy\",metrics=\"accuracy\")\n",
    "\n",
    "history = cybertron.fit(\n",
    "    x_train, y_train, batch_size = 32, epochs = 5, validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4430fca-4018-48b2-be80-5c549b51360a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9400eac0-f537-465c-a595-4239603525cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d662c834-18bc-4c79-ad80-b86e807b54e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfenv] *",
   "language": "python",
   "name": "conda-env-tfenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
